{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientia-Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from marshal import dumps, loads\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from mpstemmer import MPStemmer\n",
    "from numpy import argsort\n",
    "from polars import col, read_csv, Series, String\n",
    "from re import sub\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from types import FunctionType\n",
    "\n",
    "file_path_dataset = \"temp/repository_pnj_20212023.csv\"\n",
    "file_path_select_feature = \"temp/01_select_feature.csv\"\n",
    "file_path_lowercase = \"temp/02_lowercase.csv\"\n",
    "file_path_remove_empty_abstract = \"temp/03_remove_empty_abstract.csv\"\n",
    "file_path_remove_dash_abstract = \"temp/04_remove_dash_abstract.csv\"\n",
    "file_path_remove_same_title_abstract = \"temp/05_remove_same_title_abstract.csv\"\n",
    "file_path_merge = \"temp/06_merge.csv\"\n",
    "file_path_remove_non_alphabet = \"temp/07_remove_non_alphabet.csv\"\n",
    "file_path_unique = \"temp/08_unique.csv\"\n",
    "file_path_add_lang = \"temp/09_add_lang.csv\"\n",
    "file_path_filter_lang = \"temp/10_filter_lang.csv\"\n",
    "file_path_stem = \"temp/11_stem.csv\"\n",
    "file_path_remove_stopwords = \"temp/12_remove_stopwords.csv\"\n",
    "file_path_dataset_clean = \"temp/repository_pnj_20212023clean.csv\"\n",
    "file_path_model = \"temp/model.marshall\"\n",
    "\n",
    "\n",
    "def print_file_table(output):\n",
    "    print(f\"./{output}\", read_csv(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Select Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_csv(file_path_dataset).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = file_path_dataset\n",
    "output = file_path_select_feature\n",
    "\n",
    "read_csv(input).with_columns(\n",
    "    [col(\"title\").alias(\"f1\"), col(\"abstract\").alias(\"f2\")]\n",
    ").write_csv(output)\n",
    "\n",
    "print_file_table(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = file_path_select_feature\n",
    "output = file_path_lowercase\n",
    "\n",
    "read_csv(input).with_columns(\n",
    "    [\n",
    "        col(\"f1\").str.to_lowercase(),\n",
    "        col(\"f2\").str.to_lowercase(),\n",
    "    ]\n",
    ").write_csv(output)\n",
    "\n",
    "print_file_table(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Remove Empty Abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = file_path_lowercase\n",
    "output = file_path_remove_empty_abstract\n",
    "\n",
    "read_csv(input).with_columns(col(\"f2\").replace(\"\", None)).drop_nulls().write_csv(output)\n",
    "\n",
    "print_file_table(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Remove Dash Abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = file_path_remove_empty_abstract\n",
    "output = file_path_remove_dash_abstract\n",
    "\n",
    "read_csv(input).with_columns(col(\"f2\").replace(\"-\", None)).drop_nulls().write_csv(\n",
    "    output\n",
    ")\n",
    "\n",
    "print_file_table(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Remove Same Title Abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = file_path_remove_dash_abstract\n",
    "output = file_path_remove_same_title_abstract\n",
    "\n",
    "read_csv(input).filter(col(\"f1\") != col(\"f2\")).write_csv(output)\n",
    "\n",
    "print_file_table(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Merge Title Abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = file_path_remove_same_title_abstract\n",
    "output = file_path_merge\n",
    "\n",
    "read_csv(input).with_columns((col(\"f1\") + \" \" + col(\"f2\")).alias(\"f\")).drop(\n",
    "    [\"f1\", \"f2\"]\n",
    ").write_csv(output)\n",
    "\n",
    "print_file_table(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Remove Non Alphabetical Characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = file_path_merge\n",
    "output = file_path_remove_non_alphabet\n",
    "\n",
    "read_csv(input).with_columns(col(\"f\").str.replace_all(r\"[^a-z]+\", \" \")).write_csv(\n",
    "    output\n",
    ")\n",
    "\n",
    "print_file_table(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Remove Duplicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = file_path_remove_non_alphabet\n",
    "output = file_path_unique\n",
    "\n",
    "read_csv(input).unique(\"f\").write_csv(output)\n",
    "\n",
    "print_file_table(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Filter Based on Language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. Add Column Language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = file_path_unique\n",
    "output = file_path_add_lang\n",
    "\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "read_csv(input).with_columns(\n",
    "    col(\"f\").map_elements(detect_language, return_dtype=String).alias(\"lang\")\n",
    ").write_csv(output)\n",
    "\n",
    "print_file_table(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. Remove Non-\"Bahasa Indonesia\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = file_path_add_lang\n",
    "output = file_path_filter_lang\n",
    "\n",
    "read_csv(input).filter(col(\"lang\") == \"id\").drop(\"lang\").write_csv(output)\n",
    "\n",
    "print_file_table(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = file_path_filter_lang\n",
    "output = file_path_stem\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    return MPStemmer().stem_kalimat(text)\n",
    "\n",
    "\n",
    "read_csv(input).with_columns(\n",
    "    col(\"f\").map_elements(stem_text, return_dtype=String)\n",
    ").write_csv(output)\n",
    "\n",
    "print_file_table(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Remove Stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = file_path_stem\n",
    "output = file_path_remove_stopwords\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return StopWordRemoverFactory().create_stop_word_remover().remove(text)\n",
    "\n",
    "\n",
    "read_csv(input).with_columns(\n",
    "    col(\"f\").map_elements(remove_stopwords, return_dtype=String)\n",
    ").write_csv(output)\n",
    "\n",
    "print_file_table(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = file_path_remove_stopwords\n",
    "output = file_path_dataset_clean\n",
    "\n",
    "read_csv(input).sort(\"url\").write_csv(output)\n",
    "\n",
    "print_file_table(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset from a CSV file\n",
    "df = read_csv(file_path_dataset_clean)\n",
    "\n",
    "# Initialize a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the TF-IDF model to the 'f' column of the dataset\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"f\"].to_list())\n",
    "\n",
    "\n",
    "def find_similar_documents(title: str, abstract: str, top_n: int):\n",
    "    # Combine title and abstract into one string and convert to lowercase\n",
    "    combined_text = (title + \" \" + abstract).lower()\n",
    "\n",
    "    # Remove all non-alphabetic characters and replace them with spaces\n",
    "    cleaned_text = sub(r\"[^a-z]\", \" \", combined_text)\n",
    "\n",
    "    # Split the cleaned text into words and rejoin to remove extra spaces\n",
    "    trimmed_text = \" \".join(cleaned_text.split())\n",
    "\n",
    "    # Apply stemming to the cleaned text\n",
    "    stemmed_text = MPStemmer().stem_kalimat(trimmed_text)\n",
    "\n",
    "    # Remove stop words from the stemmed text\n",
    "    processed_text = (\n",
    "        StopWordRemoverFactory().create_stop_word_remover().remove(stemmed_text)\n",
    "    )\n",
    "\n",
    "    # Compute cosine similarity between the processed text and the TF-IDF matrix\n",
    "    cosine_scores = cosine_similarity(\n",
    "        vectorizer.transform([processed_text]), tfidf_matrix\n",
    "    ).flatten()\n",
    "\n",
    "    # Get the indices of the top N most similar documents\n",
    "    top_n_indices = argsort(cosine_scores)[-top_n:][::-1]\n",
    "\n",
    "    # Return the top N most similar documents along with their similarity scores\n",
    "    return (\n",
    "        df[top_n_indices]\n",
    "        .with_columns(\n",
    "            Series(name=\"similarity\", values=cosine_scores[top_n_indices] * 100)\n",
    "        )\n",
    "        .drop(\"f\")\n",
    "    )\n",
    "\n",
    "\n",
    "with open(file_path_model, \"wb\") as file:\n",
    "    file.write(dumps(find_similar_documents.__code__))\n",
    "\n",
    "print(f\"Model saved to ./{file_path_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path_model, \"rb\") as file:\n",
    "    model = file.read()\n",
    "\n",
    "print(\n",
    "    FunctionType(loads(model), globals())(\n",
    "        title=\"APLIKASI VARIABLE SPEED DRIVE ATV610U75N4 PADA KONTROL MOTOR AC 3 FASA BERBASIS PLC\",\n",
    "        abstract=\"Penggunaan Variable Speed Drive (VSD) pada motor induksi tiga fasa dapat mengurangi konsumsi energi yang dibutuhkan oleh peralatan secara signifikan. Pengaturan kecepatan motor induksi dapat dilakukan dengan cara mengatur tegangan sumber atau frekuensi sumber yang dimaksudkan untuk mendapatkan kecepatan putaran dan torsi motor yang diinginkan atau sesuai dengan kebutuhan. Dengan pengaplikasian Variable Speed Drive (VSD) kecepatan motor dapat dikontrol dan beroperasi dengan mode multi speed. Panel kontrol kecepatan motor ini digunakan untuk memantau dan mengatur kecepatan motor induksi tiga fasa dengan komponen utama yang terdiri dari inverter / VSD tipe ATV610U75N4, PLC, SCADA, dan motor induksi sebagai output. Inverter atau VSD (Variable Speed Drive) digunakan sebagai komponen pengatur kecepatan operasi motor induksi tiga fasa dengan mengatur frekuensi keluaran. PLC sebagai pengontrol urutan dan mengatur input output yang kemudian diproses untuk menghasilkan output yang diinginkan. Sedangkan SCADA digunakan sebagai pengendali jarak jauhnya. Pengaturan kecepatan motor dilakukan dengan mengatur besar frekuensi di inverter. Semakin besar nilai frekuensi maka putaran motor akan lebih cepat. Pada VSD tipe ATV610U75N4 terdapat jenis gangguan fasa loss (Output Phase Loss). Gangguan OPL dari inverter dapat membuat motor tidak dapat bekerja karena daya motor yang tidak memenuhi batas pengaturan (setting) pada inverter yaitu 1,5 kW sedangkan motor yang digunakan sebesar 0,25 kW. Kata Kunci : Frekuensi, Motor, PLC, SCADA, VSD\",\n",
    "        top_n=1000,\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promecarus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
